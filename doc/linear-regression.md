### 线性回归

 	线性回归是统计学中常用的算法之一。当希望表达两个变量之间的数学关系时就可以使用线性回归。当使用时，首先假设输出变量（因变量）和预测变量（自变量）之间存在线性关系。当然这种线性关系也可能存在于一个输出变量和多个预测变量之间。

​	在线性回归中，数据采用线性函数的方式进行数据建模，对模型中未知的参数也采用数据进行估计。对于一个多变量的线性回归模型可以用以下公式表示
$$
Y_1 = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ···+ \beta_p X_{ip} + \epsilon_i,i=1,2,..n
$$
其中，$Y$是$X$的线性函数，$\epsilon_i$ 是误差项。线性则是$Y_i$的条件均值，在参数 $\beta $ 里是线性的。有些函数模型看起来并不一定是线性回归 但是通过代数转换可以转换为线性回归模型。对于一个简单的线性回归，可以表示为如下：
$$
Y_i = \alpha + \beta x_i
$$
对于单变量的线性回归，是目前最容易理解的线性模型，其回归式如下：

$$
Y = \alpha + \beta x + \epsilon
$$
线性回归是回归分析中最为广泛使用的模型， 在结果预测及函数关系的应用中较为频繁。

对于线性回归算法，我们希望从训练数据中学习到线性回归方程，即：
$$
y = b + \sum_{i=1}^n w_ix_i
$$
其中，$b$ 称为偏置，$w_i$为回归系数。对于上式，令$x_i=0$ 则上式可以表示为：
$$
y = \sum_{i=n}^n w_i x_1
$$


在线性回归模型中，其目的是 求出线性模型回归方程，即求出线性回归方程中的回归系数$\omega_i​$。线性回归的评价是指如何度量预测值（Prediction）于标签（Label）之间的接近程序，线性回归模型的损失函数可以是绝对损失（Absolute Loss）或者平方损失（Squared Loss）。其中，绝对损失函数为：
$$
l = |y - \hat y|
$$
$\hat y ​$ 为预测值，且$\hat y = \sum_{i=0}^n w_i x_i​$

平方损失函数为
$$
l = (y - \hat y)^2
$$
由于平方损失处处可导，通常使用平方误差作为线性回归模型的损失函数。假设有$m$个训练样本，每个样本中有$n-1$个特征，则平方误差可以表示为：
$$
l = \frac{1}{2} \sum_{i=1}^n (y^i - \sum_{j=0}^{n-1} w_j x_j^i)^2
$$
对于如上的损失函数，线性回归的求解是希望求得平方误差最小值。